{"s": {"parameters": ["scope", "name"], "comments": "# relationships. "}, "_calc_weighted_context": {"parameters": ["model", "encoder_outputs_transposed", "encoder_output_dim", "attention_weights_3d", "scope", ""], "comments": ""}, "_calc_attention_weights": {"parameters": ["model", "attention_logits_transposed", "scope", "encoder_lengths=None", ""], "comments": ""}, "_calc_attention_logits_from_sum_match": {"parameters": ["model", "decoder_hidden_encoder_outputs_sum", "encoder_output_dim", "scope", ""], "comments": ""}, "_apply_fc_weight_for_sum_match": {"parameters": ["model", "input", "dim_in", "dim_out", "scope", "name", ""], "comments": ""}, "apply_recurrent_attention": {"parameters": ["model", "encoder_output_dim", "encoder_outputs_transposed", "weighted_encoder_outputs", "decoder_hidden_state_t", "decoder_hidden_state_dim", "attention_weighted_encoder_context_t_prev", "scope", "encoder_lengths=None", ""], "comments": ""}, "apply_regular_attention": {"parameters": ["model", "encoder_output_dim", "encoder_outputs_transposed", "weighted_encoder_outputs", "decoder_hidden_state_t", "decoder_hidden_state_dim", "scope", "encoder_lengths=None", ""], "comments": ""}, "apply_dot_attention": {"parameters": ["model", "encoder_output_dim", "#[batch_size", "encoder_output_dim", "encoder_length]encoder_outputs_transposed", "#[1", "batch_size", "decoder_state_dim]decoder_hidden_state_t", "decoder_hidden_state_dim", "scope", "encoder_lengths=None", ""], "comments": ""}, "apply_soft_coverage_attention": {"parameters": ["model", "encoder_output_dim", "encoder_outputs_transposed", "weighted_encoder_outputs", "decoder_hidden_state_t", "decoder_hidden_state_dim", "scope", "encoder_lengths", "coverage_t_prev", "coverage_weights", ""], "comments": ""}}