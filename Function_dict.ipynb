{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fndict(filename, dct): \n",
    "    #Takes .txt filename input \"example.txt\" and a dictionary, dct. If first time running, pass in empty dictionary\n",
    "    file = open(filename, \"r\")\n",
    "    \n",
    "    #Identify the header, scan for the name and parameters\n",
    "    lines = file.readlines() \n",
    "  \n",
    "    for line in lines: \n",
    "        if 'def ' in line:\n",
    "            #run search for comments function\n",
    "            #run search for parameters function\n",
    "            #save to params and comments\n",
    "            #add to dct\n",
    "            \n",
    "            if spaceOnly(line.split('def ')[0]):\n",
    "            \n",
    "                function_name = (line.split('def ')[1]).split('(')[0]\n",
    "                (params, i) = searchParams(lines, lines.index(line)) \n",
    "    \n",
    "                #Search lines above and below to identify comments, store the comments into a string and add do dict\n",
    "                comments = searchComments(lines, i, 3)\n",
    "    \n",
    "                dct[function_name] = {'parameters' : params, 'comments' : comments}\n",
    "    \n",
    "    #dump dict into a json at the end\n",
    "    prefix = filename.replace('.txt','')\n",
    "    print(dct)\n",
    "    with open(prefix + 'dict.json', 'w') as f:\n",
    "        json.dump(dct, f)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spaceOnly(str):\n",
    "#This function is just used to check that 'def' is a function header and not in a comment\n",
    "    for i in range(len(str)):\n",
    "        if str[i] != ' ':\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchParams(lines, idx):\n",
    "    #scan lines starting at idx until ')' is reached, take params separated by ',' using split\n",
    "    #if no '(' on that line, move to next line and concat it to line\n",
    "    line = lines[idx]\n",
    "    inputs = \"\"\n",
    "    end = idx\n",
    "    for i in range(idx, len(lines)):\n",
    "        ln = lines[i]\n",
    "        ln = ln.strip('\\n')\n",
    "        ln = ln.strip('\\t')\n",
    "        inputs = inputs + ln\n",
    "        if ')' in lines[i]:\n",
    "            end = i\n",
    "            break\n",
    "    inputs = inputs.replace(' ', '')\n",
    "    #print(end)\n",
    "    lst = (inputs.split('(')[1]).split(')')[0]\n",
    "    param_list = lst.split(',')\n",
    "    return (param_list, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchComments(lines, idx, threshold):\n",
    "    #scan lines within threshold lines below the header for start of comments beginning with \"\"\", ''', or # \n",
    "    #makes assumption that no comments are above header\n",
    "    #identifies by scanning for start of header\n",
    "    comments = ''\n",
    "    for i in range(threshold + 1):\n",
    "        #case if comments begin with three '\n",
    "        if \"'''\" in lines[idx + i]:\n",
    "            if lines[idx + i].count(\"'''\") > 1:\n",
    "                #print('enter')\n",
    "                ln = lines[idx + i].strip(\"'''\")\n",
    "                ln = ln.strip('\\n')\n",
    "                ln = ln.strip('\\t')\n",
    "                comments = comments + ln + ' '\n",
    "                #print(ln)\n",
    "            else:\n",
    "                ln = lines[idx + i].strip(\"'''\")\n",
    "                ln = ln.strip('\\n')\n",
    "                ln = ln.strip('\\t')\n",
    "                comments = comments + ln + ' '\n",
    "                idx2 = idx + i + 1\n",
    "                while not \"'''\" in lines[idx2]:\n",
    "                    #print(lines[idx2])\n",
    "                    s = lines[idx2]\n",
    "                    s = s.strip('\\n')\n",
    "                    s = s.strip('\\t')\n",
    "                    comments = comments + s + ' '\n",
    "                    idx2 = idx2 + 1\n",
    "                    if idx2 >= len(lines):\n",
    "                        break\n",
    "            comments = comments.replace('  ', '')\n",
    "            break\n",
    "        \n",
    "        #case if comments begin with three \"\n",
    "        if '\"\"\"' in lines[idx + i]:\n",
    "            if lines[idx + i].count('\"\"\"') > 1:\n",
    "                ln = lines[idx + i].strip('\"\"\"')\n",
    "                ln = ln.strip('\\n')\n",
    "                ln = ln.strip('\\t')\n",
    "                comments = comments + ln + ' '\n",
    "            else:\n",
    "                ln = lines[idx + i].strip('\"\"\"')\n",
    "                ln = ln.strip('\\n')\n",
    "                ln = ln.strip('\\t')\n",
    "                comments = comments + ln + ' '\n",
    "                idx2 = idx + i + 1\n",
    "                while not '\"\"\"' in lines[idx2]:\n",
    "                    #print(lines[idx2])\n",
    "                    s = lines[idx2]\n",
    "                    s = s.strip('\\n')\n",
    "                    s = s.strip('\\t')\n",
    "                    comments = comments + s + ' '\n",
    "                    idx2 = idx2 + 1\n",
    "                    if idx2 >= len(lines):\n",
    "                        break\n",
    "            comments = comments.replace('  ', '')\n",
    "            break    \n",
    "        \n",
    "        #case if comments begin with #\n",
    "        if '#' in lines[idx + i]:\n",
    "            idx2 = idx + i + 1\n",
    "            while '#' in lines[idx2]:\n",
    "                #print(lines[idx2])\n",
    "                s = lines[idx2]\n",
    "                s = s.strip('\\n')\n",
    "                s = s.strip('\\t')\n",
    "                s = s.strip('#')\n",
    "                comments = comments + s + ' '\n",
    "                idx2 = idx2 + 1\n",
    "                if idx2 >= len(lines):\n",
    "                    break\n",
    "            comments = comments.replace('  ', '')\n",
    "            break\n",
    "    return comments\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fun1': {'parameters': ['param1', 'param2'], 'comments': \"''' test comment line 1 test comment line 2 test comment line 3 test comment line 4 \"}, 'fun2': {'parameters': ['param3'], 'comments': '\"\"\" egg 1 egg 2 '}, 'fun3 ': {'parameters': [''], 'comments': \"'''I am very hungry''' \"}, 'fun4': {'parameters': ['param4', 'param5', 'param6', 'param7', 'param8'], 'comments': 'dog cow horse pig mouse '}, 'fun5 ': {'parameters': ['param8', 'param9', 'param10', 'param11'], 'comments': '\"\"\" hello this is a a test line k '}}\n"
     ]
    }
   ],
   "source": [
    "fndict('FunDict_tc1.txt', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'InitOpsLibrary': {'parameters': ['name', ' trigger_lazy=True'], 'comments': '\"\"\"Loads a dynamic library that contains custom operators into Caffe2. Since Caffe2 uses static variable registration, you can optionally load a separate .so file that contains custom operators and registers that into the caffe2 core binary. In C++, this is usually done by either declaring dependency during compilation time, or via dynload. This allows us to do registration similarly on the Python side. Args: name: a name that ends in .so, such as \"my_custom_op.so\". Otherwise, the command will simply be ignored. Returns: None '}, 'GetImportedOpsLibraries': {'parameters': [''], 'comments': ''}, '_init_impl': {'parameters': ['path', ' trigger_lazy=True'], 'comments': ''}}\n"
     ]
    }
   ],
   "source": [
    "fndict('dyndep.txt', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__init__': {'parameters': ['self', ' arg_name=None', ' allow_default=False'], 'comments': ''}, '_stack': {'parameters': ['self'], 'comments': ''}, 'enter': {'parameters': ['self', ' value'], 'comments': ''}, 'exit': {'parameters': ['self', ' value'], 'comments': ''}, 'get_active': {'parameters': ['self', ' required=True'], 'comments': ''}, 'register': {'parameters': ['self', ' ctx_info'], 'comments': ''}, 'get': {'parameters': ['self', ' cls'], 'comments': ''}, '_context_registry': {'parameters': [''], 'comments': ''}, '__enter__': {'parameters': ['self'], 'comments': ''}, '__exit__': {'parameters': ['self', ' *args'], 'comments': ''}, '__call__': {'parameters': ['self', ' cls'], 'comments': ''}, 'wrapper': {'parameters': ['*args', ' **kwargs'], 'comments': ''}, '_current': {'parameters': ['cls', ' value=None', ' required=True'], 'comments': ''}, '_get_active_context': {'parameters': ['cls', ' val=None', ' required=True'], 'comments': ''}}\n"
     ]
    }
   ],
   "source": [
    "fndict('context.txt', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gen_do_gradient': {'parameters': ['op', 'g_output'], 'comments': '\"\"\" Generates gradient Do operator, given forward Do op and a list of gradient blobs corresponding to forward op\\'s outputs Returns a gradient op and a list of blobs corresponding to input gradients '}, 'dedupe_g_output': {'parameters': ['op', 'g_output'], 'comments': '# blob corresponding to different forward op output blobs, Do operator # requires a bijection between inner and outer names, make sure we do # deduplication '}, 'gen_while_gradient': {'parameters': ['op', 'g_output'], 'comments': '\"\"\" Generates gradient While operator '}, '_prepare_gradient_while_ops': {'parameters': ['fwd_op', 'input_names', 'output_names', 'loop_grad_net', 'workspace_blob', 'init_grad_map', 'loop_grad_map'], 'comments': ''}, '_get_do_arguments': {'parameters': ['do_op'], 'comments': ''}, 'gen_if_gradient': {'parameters': ['op', 'g_output'], 'comments': '\"\"\" Generates gradient If operator, given forward If op and a list of gradient blobs corresponding to forward op\\'s outputs Returns a gradient op and a list of blobs corresponding to input gradients '}, '_gen_subnet_gradient': {'parameters': ['subnet', 'init_grad'], 'comments': ''}, '_get_net_argument': {'parameters': ['op', 'net_name'], 'comments': ''}, 'getNetArgument': {'parameters': ['op', 'net_name'], 'comments': '\"\"\"A wrapper for external call\"\"\" '}, '_gen_subgradient_pass': {'parameters': ['subnet', 'init_grad'], 'comments': ''}, '_do_op_sanity_check_and_process': {'parameters': ['op'], 'comments': ''}, '_prepare_blob_copy_op': {'parameters': ['from_name', 'to_name'], 'comments': ''}, '_prepare_gradient_do_op': {'parameters': ['fwd_op', 'fwd_net', 'grad_ops', 'inputs', 'outputs', 'blob_bindings', 'saved_fwd_blobs', 'workspace_blob_name'], 'comments': ''}, '_gen_grad_zero_init_ops': {'parameters': ['init_grad_map', 'grad_map', 'grad_output_names'], 'comments': '# so that grad_output has the same shape '}, '_prepare_gradient_if_op': {'parameters': ['fwd_op', 'input_names', 'output_names', 'then_grad_net', 'else_grad_net'], 'comments': ''}, 'disambiguate_grad_if_op_output': {'parameters': ['grad_op', 'idx', 'new_grad_output'], 'comments': ''}}\n"
     ]
    }
   ],
   "source": [
    "fndict('control_ops_grad.txt', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'share_grad_blobs': {'parameters': ['net', 'losses', 'param_grads', 'namescope', 'dont_share_blobs=None', 'share_activations=False', 'blob_shapes=None', ''], 'comments': \"''' Implements similar optimization as Torch's shareGradInput(): for the gradients that are passed between layers, share blobs between operators when possible. This yields significant memory savings with deep networks.Returns an optimized protobuf (assign to net._net) \"}, 'is_grad_blob': {'parameters': ['b'], 'comments': '# to handle the auto-split gradients '}, 'is_grad_op': {'parameters': ['op'], 'comments': ''}, 'optimize_inference_for_dag': {'parameters': ['net', 'input_blobs', 'namescope=\"\"'], 'comments': ''}, 'is_activation_blob': {'parameters': ['b'], 'comments': ''}, 'estimate_memory_usage': {'parameters': ['protos', 'shapes', 'types', 'devicescope'], 'comments': \"''' Estimate memory usage of a model. This is an estimate because we assume a single threaded execution and miss some internal memory usage of operators. Only estimates the memory for a given device scope.Also, currently it does not handle correctly if blob sizes vary during execution, as it uses only the final blob size.Returns (total, highwater, by op type) memory allocation in bytes. \"}, 'split_net': {'parameters': ['proto'], 'comments': ''}, 'num_bytes': {'parameters': ['blob'], 'comments': ''}, 'release_blobs_when_used': {'parameters': ['netproto', 'dont_free_blobs', 'selector_fun=None'], 'comments': '\\'\\'\\' Insert Free-ops after a blob has been used the last time, so that its memory can be reclaimed. Use this only with efficient caching memory managers (such as CUB, --caffe2_cuda_memory_pool=cub).Blobs used with Alias op won\\'t be freed.@dont_free_blobs:is a set of blobs that should not be freed @selector_fun: optional lambda that return True if blob namecan be released. Use for easy special filtering, likeexcluding blobs with \"loss\" in the name.Returns a new protobuffer. To use with a model, use: model.net._net = memonger.release_blobs_when_used(..) '}, '_find_source_nodes': {'parameters': ['g'], 'comments': \"''' Return nodes without predecessors ''' \"}, '_find_target_nodes': {'parameters': ['g'], 'comments': \"''' Return nodes without successors ''' \"}, '_add_single_target_ifneeded': {'parameters': ['g'], 'comments': ''}, '_next_available_idx': {'parameters': ['g'], 'comments': ''}, '_get_path': {'parameters': ['pred_list', 'dist_list'], 'comments': \"''' Get the path from nx.bellman_ford()'s output ''' \"}, '_get_longest_paths': {'parameters': ['g', 'source_nodes'], 'comments': \"''' Get the longest path for nodes in 'source_nodes' Find with bellman_ford() by setting weight = -1 \"}, '_build_tree': {'parameters': ['paths'], 'comments': \"''' Build a tree for given paths based on common elements. Last elements of all paths are the same, which is the root of the tree. \"}, '_compute_tree_height': {'parameters': ['g', 'root'], 'comments': \"''' Compute the heights of the tree for all nodes Height of leaves are 0 \"}, '_get_height': {'parameters': ['root'], 'comments': ''}, '_sort_tree_leaves': {'parameters': ['g', 'root'], 'comments': \"''' For each node, sort its child nodes based on the height of the nodes. Return the leaf nodes of the tree after sorting. \"}, '_get_sorted_leaves': {'parameters': ['root'], 'comments': ''}, 'topological_sort_traversal_longest_path': {'parameters': ['g'], 'comments': \"''' The graph 'g' may contain several source nodes (nodes without incoming edge), which could be in any order and still be a valid topological sorting result. We would like to arrange these source nodes so that the average live spans of the computed blobs are shorter. The idea is to sort the source nodes based on the length of their path to the target node so that the one with longer path is used first. This is done by: - Add a single target node if there are multiple target nodes in 'g'. - Find the longest path between each source and the target node. - Convert the longest paths to a tree with the target node being the root and source nodes being the leaves. - Sort the nodes of the tree based on the height of the tree. \"}, 'topological_sort_traversal': {'parameters': ['g'], 'comments': ''}, 'compute_ranges': {'parameters': ['linearized_ops', 'blob_sizes=None'], 'comments': ''}, 'is_compatible': {'parameters': ['candidate_range', 'assignment', 'static_blobs'], 'comments': ''}, 'compute_blob_assignments': {'parameters': ['assignments'], 'comments': ''}, '_get_max_size': {'parameters': ['assignment'], 'comments': ''}, 'get_memory_usage': {'parameters': ['assignments'], 'comments': ''}, 'compute_assignments_greedy': {'parameters': ['ranges_sorted', 'init_assignments=None'], 'comments': ''}, '_get_count': {'parameters': ['assignments'], 'comments': \"''' Return number of blobs in assignments ''' \"}, 'compute_assignments_dp': {'parameters': ['ranges_sorted', 'init_assignment', 'counter=None'], 'comments': \"''' Compute assignment for blobs in 'ranges_sorted' on top of 'init_assignment' using dynamic programming + recursion.ranges_sorted: blobs sorted by 'used' init_assignment: assignment to start with, blobs in 'ranges_sorted' shouldnot be used in 'init_assignment'Using f(b, k, init) to represent the best assignment for blobs b[0:k] given initial assignment 'init', we have f(b, k, init) = f(b, j, init) + find_best(b[j:k], f(b, j, init)) where j is the index of the last best assignment that is independent of blob b[k - 1] (b[k - 1] is compatible with all assignments in f(b, j, init)), and find_best(b1, init1) gives the best assignment for blobs in 'b1' based on the initial assignment 'init1', and blobs b1[0:-1] should be incompatible with b1[-1]. f(b, len(b), []) gives the best assignment for blobs 'b'.For find_best(b, init), since b[0:-1] are not compatible with b[-1], we could reduce it to a smaller problem to find best assignment for b[0:-1] as find_best(b, init) = min { f(b[0:-1], len(b) - 1, init - x) + [x, b[-1]] for x in init, or f(b[0:-1], len(b) - 1, init) + [b[-1]] } where min{} gives the assignment with minimum memory usage. \"}, '_get_compatible_prev': {'parameters': ['candidate_range', 'best_assignments', 'cur_idx'], 'comments': \"''' Find closest position k of best_assignments that is independent of candidate_range that candiate_range is compatible with all assignments in best_assignments[k]. Return -1 if not found. \"}, 'is_compatible_all': {'parameters': ['candidate_range', 'assignments'], 'comments': \"''' return true if compatible for all assignments in assignments ''' \"}, '_find_best': {'parameters': ['ranges', 'init_assignment', 'prev_best_assignment', 'counter'], 'comments': \"''' Find the best assignment for blobs 'ranges' given an initialized assignment 'init_assignment'.Blobs in ranges[0:-1] should be incompatible with blob range[-1]. 'prev_best_assignment': best assignment for blobs in ranges[:-1]By assigning ranges[-1] to each assignment k in 'init_assignment' or in a new assignment, the problem becomes a smaller problem to find the best assignment for ranges[0:-1] given the initial assignment init_assigment[0:k, (k+1):-1]. \"}, 'get_updated_ranges': {'parameters': ['ranges', 'max_live=None'], 'comments': \"''' Set LiveRange.defined = -1 if it is None Set LiveRange.used = max_live if it is None Set LiveRanee.size = 1 if it is None \"}, '_get_max_live': {'parameters': ['ranges'], 'comments': ''}, '_update_range': {'parameters': ['x', 'max_live', 'size'], 'comments': ''}, 'compute_assignments': {'parameters': ['ranges', 'static_blobs', 'algo'], 'comments': \"''' algo: Method used to find assignments (AssignmentAlgorithm.GREEDY or AssignmentAlgorithm.DYNAMIC_PROGRAMMING). AssignmentAlgorithm.DYNAMIC_PROGRAMMING gives optimal solution at the cost of more computation. AssignmentAlgorithm.GREEDY may be better in the case 'blob_sizes' is not provided. \"}, 'verify_assignments': {'parameters': ['assignments'], 'comments': ''}, 'compute_interference_graph': {'parameters': ['ops'], 'comments': ''}, 'apply_assignments': {'parameters': ['net', 'blob_assignments'], 'comments': ''}, 'canonical_name': {'parameters': ['blob'], 'comments': ''}, 'apply_recurrent_blob_assignments': {'parameters': ['op', 'blob_assignments', 'canonical_name'], 'comments': ''}, 'optimize_inference_fast': {'parameters': ['net', 'static_blobs'], 'comments': ''}, 'optimize_interference': {'parameters': ['net', 'static_blobs', 'ordering_function=topological_sort_traversal', 'blob_sizes=None', 'algo=AssignmentAlgorithm.GREEDY'], 'comments': '\"\"\" ordering_function: topological_sort_traversal ortopological_sort_traversal_longest_path.topological_sort_traversal_longest_path gives betterresults but needs a bit more computation. algo: Method used to find assignments (AssignmentAlgorithm.GREEDY or AssignmentAlgorithm.DYNAMIC_PROGRAMMING). AssignmentAlgorithm.DYNAMIC_PROGRAMMING gives optimal solution at the cost of more computation. AssignmentAlgorithm.GREEDY may be better in the case \\'blob_sizes\\' is not provided. '}, 'verify_inplace_blobs': {'parameters': ['net_a', 'net_b'], 'comments': '\"\"\" Verifies that net_a and net_b have the same in-place blob assignments. Particularly, that memonger did not add an in-place assignment when that did not exist before. '}, 'get_inplaces': {'parameters': ['op'], 'comments': ''}, 'verify_graph_equality': {'parameters': ['net_a', 'net_b'], 'comments': '\"\"\" Determines if the execution of two graphs are identical. That is, all inputs blobs are mapped to the same output blobs for each operator in their respective positions.This is meant to check the output of memonger with the original graph. It assumes that the nets have same external input and output.O(E) runtime + O(1) amortized cost to hash for python dict '}, 'parent_list': {'parameters': ['ops'], 'comments': ''}, 'blob_nbytes': {'parameters': ['blob'], 'comments': ''}, 'compute_statistics': {'parameters': ['assignments'], 'comments': ''}, 'collect_blob_sizes': {'parameters': ['net'], 'comments': ''}}\n"
     ]
    }
   ],
   "source": [
    "fndict('memonger.txt', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s': {'parameters': ['scope', 'name'], 'comments': '# relationships. '}, '_calc_weighted_context': {'parameters': ['model', 'encoder_outputs_transposed', 'encoder_output_dim', 'attention_weights_3d', 'scope', ''], 'comments': ''}, '_calc_attention_weights': {'parameters': ['model', 'attention_logits_transposed', 'scope', 'encoder_lengths=None', ''], 'comments': ''}, '_calc_attention_logits_from_sum_match': {'parameters': ['model', 'decoder_hidden_encoder_outputs_sum', 'encoder_output_dim', 'scope', ''], 'comments': ''}, '_apply_fc_weight_for_sum_match': {'parameters': ['model', 'input', 'dim_in', 'dim_out', 'scope', 'name', ''], 'comments': ''}, 'apply_recurrent_attention': {'parameters': ['model', 'encoder_output_dim', 'encoder_outputs_transposed', 'weighted_encoder_outputs', 'decoder_hidden_state_t', 'decoder_hidden_state_dim', 'attention_weighted_encoder_context_t_prev', 'scope', 'encoder_lengths=None', ''], 'comments': ''}, 'apply_regular_attention': {'parameters': ['model', 'encoder_output_dim', 'encoder_outputs_transposed', 'weighted_encoder_outputs', 'decoder_hidden_state_t', 'decoder_hidden_state_dim', 'scope', 'encoder_lengths=None', ''], 'comments': ''}, 'apply_dot_attention': {'parameters': ['model', 'encoder_output_dim', '#[batch_size', 'encoder_output_dim', 'encoder_length]encoder_outputs_transposed', '#[1', 'batch_size', 'decoder_state_dim]decoder_hidden_state_t', 'decoder_hidden_state_dim', 'scope', 'encoder_lengths=None', ''], 'comments': ''}, 'apply_soft_coverage_attention': {'parameters': ['model', 'encoder_output_dim', 'encoder_outputs_transposed', 'weighted_encoder_outputs', 'decoder_hidden_state_t', 'decoder_hidden_state_dim', 'scope', 'encoder_lengths', 'coverage_t_prev', 'coverage_weights', ''], 'comments': ''}}\n"
     ]
    }
   ],
   "source": [
    "fndict('attention.txt', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__init__': {'parameters': ['self', 'name'], 'comments': '\"\"\"Initializes a Trie object.\"\"\" '}, 'GetSymbolTrie': {'parameters': ['target', 'nm_command', 'max_depth'], 'comments': '\"\"\"Gets a symbol trie with the passed in target. Args: target: the target binary to inspect. nm_command: the command to run nm. max_depth: the maximum depth to create the trie. '}, 'MaybeAddColor': {'parameters': ['s', 'color'], 'comments': '\"\"\"Wrap the input string to the xterm green color, if color is set. '}, 'ReadableSize': {'parameters': ['num'], 'comments': '\"\"\"Get a human-readable size.\"\"\" '}, 'PrintTrie': {'parameters': ['trie', 'prefix', 'max_depth', 'min_size', 'color'], 'comments': '\"\"\"Prints the symbol trie in a readable manner. '}, 'main': {'parameters': ['argv'], 'comments': ''}}\n"
     ]
    }
   ],
   "source": [
    "fndict('binarysize.txt', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'forward': {'parameters': ['ctx', 'input'], 'comments': '\"\"\" In the forward pass we receive a Tensor containing the input and return a Tensor containing the output. ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. '}, 'backward': {'parameters': ['ctx', 'grad_output'], 'comments': '\"\"\" In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output, and we need to compute the gradient of the loss with respect to the input. '}}\n",
      "----------------------------------------------\n",
      "{'__init__': {'parameters': ['self'], 'comments': '\"\"\" In the constructor we instantiate four parameters and assign them as member parameters. '}, 'forward': {'parameters': ['self', 'x'], 'comments': '\"\"\" In the forward function we accept a Tensor of input data and we must return a Tensor of output data. We can use Modules defined in the constructor as well as arbitrary operators on Tensors. '}, 'string': {'parameters': ['self'], 'comments': '\"\"\" Just like any class in Python, you can also define custom method on PyTorch modules '}}\n",
      "----------------------------------------------\n",
      "{'__init__': {'parameters': ['self'], 'comments': '\"\"\" In the constructor we instantiate five parameters and assign them as members. '}, 'forward': {'parameters': ['self', 'x'], 'comments': '\"\"\" For the forward pass of the model, we randomly choose either 4, 5 and reuse the e parameter to compute the contribution of these orders.Since each forward pass builds a dynamic computation graph, we can use normal Python control-flow operators like loops or conditional statements when defining the forward pass of the model.Here we also see that it is perfectly safe to reuse the same parameter many times when defining a computational graph. '}, 'string': {'parameters': ['self'], 'comments': '\"\"\" Just like any class in Python, you can also define custom method on PyTorch modules '}}\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fndict('pytorchEx1.txt', {})\n",
    "print('----------------------------------------------')\n",
    "fndict('pytorchEx2.txt', {})\n",
    "print('----------------------------------------------')\n",
    "fndict('pytorchEx3.txt', {})\n",
    "print('----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmemonger.py - passed\\nattention.py (contains comments above header)\\ncontext.py - passed\\ncontrol_ops_grad.py - passes but has variable including \"def\", so it falsely adds \"= _prepare_gradient_do_op\" \\n    as a function in the dict, need to implement that \\ndyndep.py - passed\\nFunDict_tc1.txt - passed\\nLearning PyTorch with examples - passed but have multiple spaces between lines\\nbinarysize.txt - passed\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test cases\n",
    "'''\n",
    "memonger.py - passed\n",
    "attention.py (contains comments above header)\n",
    "context.py - passed\n",
    "control_ops_grad.py - passes but has variable including \"def\", so it falsely adds \"= _prepare_gradient_do_op\" \n",
    "    as a function in the dict, need to implement that \n",
    "dyndep.py - passed\n",
    "FunDict_tc1.txt - passed\n",
    "Learning PyTorch with examples - passed but have multiple spaces between lines\n",
    "binarysize.txt - passed\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
