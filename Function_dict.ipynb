{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fndict(filename, dct): \n",
    "    #Takes .txt filename input \"example.txt\" and a dictionary, dct. If first time running, pass in empty dictionary\n",
    "    file = open(filename, \"r\")\n",
    "    \n",
    "    #Identify the header, scan for the name and parameters\n",
    "    lines = file.readlines() \n",
    "  \n",
    "    for line in lines: \n",
    "        if 'def ' in line:\n",
    "            #run search for comments function\n",
    "            #run search for parameters function\n",
    "            #save to params and comments\n",
    "            #add to dct\n",
    "        \n",
    "            function_name = (line.split('def ')[1]).split('(')[0]\n",
    "            (params, i) = searchParams(lines, lines.index(line)) \n",
    "    \n",
    "            #Search lines above and below to identify comments, store the comments into a string and add do dict\n",
    "            comments = searchComments(lines, i, 3)\n",
    "    \n",
    "            dct[function_name] = {'parameters' : params, 'comments' : comments}\n",
    "    \n",
    "    #dump dict into a json at the end\n",
    "    prefix = filename.replace('.txt','')\n",
    "    print(dct)\n",
    "    with open(prefix + 'dict.json', 'w') as f:\n",
    "        json.dump(dct, f)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchParams(lines, idx):\n",
    "    #scan lines starting at idx until ')' is reached, take params separated by ',' using split\n",
    "    #if no '(' on that line, move to next line and concat it to line\n",
    "    line = lines[idx]\n",
    "    inputs = \"\"\n",
    "    end = idx\n",
    "    for i in range(idx, len(lines)):\n",
    "        ln = lines[i]\n",
    "        ln = ln.strip('\\n')\n",
    "        ln = ln.strip('\\t')\n",
    "        inputs = inputs + ln\n",
    "        if ')' in lines[i]:\n",
    "            end = i\n",
    "            break\n",
    "    inputs = inputs.strip(' ')\n",
    "    lst = (inputs.split('(')[1]).split(')')[0]\n",
    "    param_list = lst.split(',')\n",
    "    return (param_list, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchComments(lines, idx, threshold):\n",
    "    #scan lines within threshold lines below the header for start of comments beginning with \"\"\", ''', or # \n",
    "    #makes assumption that no comments are above header\n",
    "    #identifies by scanning for start of header\n",
    "    comments = ''\n",
    "    for i in range(threshold + 1):\n",
    "        #case if comments begin with three '\n",
    "        if \"'''\" in lines[idx + i]:\n",
    "            if lines[idx + i].count(\"'''\") > 1:\n",
    "                #print('enter')\n",
    "                ln = lines[idx + i].strip(\"'''\")\n",
    "                ln = ln.strip('\\n')\n",
    "                ln = ln.strip('\\t')\n",
    "                comments = comments + ln + ' '\n",
    "                print(ln)\n",
    "            else:\n",
    "                ln = lines[idx + i].strip(\"'''\")\n",
    "                ln = ln.strip('\\n')\n",
    "                ln = ln.strip('\\t')\n",
    "                comments = comments + ln + ' '\n",
    "                idx2 = idx + i + 1\n",
    "                while not \"'''\" in lines[idx2]:\n",
    "                    #print(lines[idx2])\n",
    "                    s = lines[idx2]\n",
    "                    s = s.strip('\\n')\n",
    "                    s = s.strip('\\t')\n",
    "                    comments = comments + s + ' '\n",
    "                    idx2 = idx2 + 1\n",
    "                    if idx2 >= len(lines):\n",
    "                        break\n",
    "            break\n",
    "        \n",
    "        #case if comments begin with three \"\n",
    "        if '\"\"\"' in lines[idx + i]:\n",
    "            if lines[idx + i].count('\"\"\"') > 1:\n",
    "                ln = lines[idx + i].strip('\"\"\"')\n",
    "                ln = ln.strip('\\n')\n",
    "                ln = ln.strip('\\t')\n",
    "                comments = comments + ln + ' '\n",
    "            else:\n",
    "                ln = lines[idx + i].strip('\"\"\"')\n",
    "                ln = ln.strip('\\n')\n",
    "                ln = ln.strip('\\t')\n",
    "                comments = comments + ln + ' '\n",
    "                idx2 = idx + i + 1\n",
    "                while not '\"\"\"' in lines[idx2]:\n",
    "                    #print(lines[idx2])\n",
    "                    s = lines[idx2]\n",
    "                    s = s.strip('\\n')\n",
    "                    s = s.strip('\\t')\n",
    "                    comments = comments + s + ' '\n",
    "                    idx2 = idx2 + 1\n",
    "                    if idx2 >= len(lines):\n",
    "                        break\n",
    "            break    \n",
    "        \n",
    "        #case if comments begin with #\n",
    "        if '#' in lines[idx + i]:\n",
    "            idx2 = idx + i + 1\n",
    "            while '#' in lines[idx2]:\n",
    "                #print(lines[idx2])\n",
    "                s = lines[idx2]\n",
    "                s = s.strip('\\n')\n",
    "                s = s.strip('\\t')\n",
    "                s = s.strip('#')\n",
    "                comments = comments + s + ' '\n",
    "                idx2 = idx2 + 1\n",
    "                if idx2 >= len(lines):\n",
    "                    break\n",
    "            break\n",
    "    return comments\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter\n",
      "'''I am very hungry'''\n",
      "{'fun1': {'parameters': ['param1', ' param2'], 'comments': \"''' test comment line 1 test comment line 2 test comment line 3 test comment line 4 \"}, 'fun2': {'parameters': ['param3'], 'comments': '\"\"\" egg 1 egg 2 '}, 'fun3 ': {'parameters': [''], 'comments': \"'''I am very hungry''' \"}, 'fun4': {'parameters': ['param4', ' param5', ' param6', ' param7', ' param8'], 'comments': 'dog cow horse pig mouse '}, 'fun5 ': {'parameters': ['param8', ' param9', ' param10', ' param11'], 'comments': '\"\"\"     hello this is a a test line k '}}\n"
     ]
    }
   ],
   "source": [
    "fndict('FunDict_tc1.txt', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'InitOpsLibrary': {'parameters': ['name', ' trigger_lazy=True'], 'comments': '    \"\"\"Loads a dynamic library that contains custom operators into Caffe2.     Since Caffe2 uses static variable registration, you can optionally load a     separate .so file that contains custom operators and registers that into     the caffe2 core binary. In C++, this is usually done by either declaring     dependency during compilation time, or via dynload. This allows us to do     registration similarly on the Python side.     Args:         name: a name that ends in .so, such as \"my_custom_op.so\". Otherwise,             the command will simply be ignored.     Returns:         None '}, 'GetImportedOpsLibraries': {'parameters': [''], 'comments': ''}, '_init_impl': {'parameters': ['path', ' trigger_lazy=True'], 'comments': ''}}\n"
     ]
    }
   ],
   "source": [
    "fndict('dyndep.txt', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__init__': {'parameters': ['self', ' arg_name=None', ' allow_default=False'], 'comments': ''}, '_stack': {'parameters': ['self'], 'comments': ''}, 'enter': {'parameters': ['self', ' value'], 'comments': ''}, 'exit': {'parameters': ['self', ' value'], 'comments': ''}, 'get_active': {'parameters': ['self', ' required=True'], 'comments': ''}, 'register': {'parameters': ['self', ' ctx_info'], 'comments': ''}, 'get': {'parameters': ['self', ' cls'], 'comments': ''}, '_context_registry': {'parameters': [''], 'comments': ''}, '__enter__': {'parameters': ['self'], 'comments': ''}, '__exit__': {'parameters': ['self', ' *args'], 'comments': ''}, '__call__': {'parameters': ['self', ' cls'], 'comments': ''}, 'wrapper': {'parameters': ['*args', ' **kwargs'], 'comments': ''}, '_current': {'parameters': ['cls', ' value=None', ' required=True'], 'comments': ''}, '_get_active_context': {'parameters': ['cls', ' val=None', ' required=True'], 'comments': ''}}\n"
     ]
    }
   ],
   "source": [
    "fndict('context.txt', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gen_do_gradient': {'parameters': ['op', ' g_output'], 'comments': '    \"\"\"     Generates gradient Do operator, given forward Do op and a list     of gradient blobs corresponding to forward op\\'s outputs     Returns a gradient op and a list of blobs corresponding to input gradients '}, '= _prepare_gradient_do_op': {'parameters': ['        fwd_op=op', '        fwd_net=subnet', '        grad_ops=inner_grad_ops', '        inputs=new_op_inputs', '        outputs=new_op_outputs', '        blob_bindings=new_blob_bindings', '        saved_fwd_blobs=saved_local_blob_names', '        workspace_blob_name=workspace_blob_name'], 'comments': ''}, 'dedupe_g_output': {'parameters': ['op', ' g_output'], 'comments': '    # blob corresponding to different forward op output blobs, Do operator     # requires a bijection between inner and outer names, make sure we do     # deduplication '}, 'gen_while_gradient': {'parameters': ['op', ' g_output'], 'comments': '    \"\"\"     Generates gradient While operator '}, '_prepare_gradient_while_ops': {'parameters': ['        fwd_op', ' input_names', ' output_names', ' loop_grad_net', ' workspace_blob', '        init_grad_map', ' loop_grad_map'], 'comments': ''}, '= caffe2_pb2.OperatorDef': {'parameters': [''], 'comments': ''}, '_get_do_arguments': {'parameters': ['do_op'], 'comments': ''}, 'gen_if_gradient': {'parameters': ['op', ' g_output'], 'comments': '    \"\"\"     Generates gradient If operator, given forward If op and a list     of gradient blobs corresponding to forward op\\'s outputs     Returns a gradient op and a list of blobs corresponding to input gradients '}, '= _prepare_gradient_if_op': {'parameters': ['        fwd_op=op', '        input_names=input_names', '        output_names=output_names', '        then_grad_net=then_grad_net', '        else_grad_net=else_grad_net'], 'comments': ''}, '_gen_subnet_gradient': {'parameters': ['subnet', ' init_grad'], 'comments': ''}, '= caffe2_pb2.NetDef': {'parameters': [''], 'comments': ''}, '_get_net_argument': {'parameters': ['op', ' net_name'], 'comments': ''}, 'getNetArgument': {'parameters': ['op', ' net_name'], 'comments': '    \"\"\"A wrapper for external call\"\"\" '}, '_gen_subgradient_pass': {'parameters': ['subnet', ' init_grad'], 'comments': ''}, '_do_op_sanity_check_and_process': {'parameters': ['op'], 'comments': ''}, '_prepare_blob_copy_op': {'parameters': ['from_name', ' to_name'], 'comments': ''}, '_prepare_gradient_do_op': {'parameters': ['        fwd_op', ' fwd_net', ' grad_ops', ' inputs', ' outputs', ' blob_bindings', ' saved_fwd_blobs', '        workspace_blob_name'], 'comments': ''}, '_gen_grad_zero_init_ops': {'parameters': ['init_grad_map', ' grad_map', ' grad_output_names'], 'comments': '        # so that grad_output has the same shape '}, '_prepare_gradient_if_op': {'parameters': ['        fwd_op', ' input_names', ' output_names', ' then_grad_net', ' else_grad_net'], 'comments': ''}, 'disambiguate_grad_if_op_output': {'parameters': ['grad_op', ' idx', ' new_grad_output'], 'comments': ''}}\n"
     ]
    }
   ],
   "source": [
    "fndict('control_ops_grad.txt', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ''' Return nodes without predecessors '''\n",
      "    ''' Return nodes without successors '''\n",
      "    ''' Get the path from nx.bellman_ford()'s output '''\n",
      "    ''' Return number of blobs in assignments '''\n",
      "            ''' return true if compatible for all assignments in assignments '''\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1c93b2b16a5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfndict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'memonger.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-9204dfbe6ccd>\u001b[0m in \u001b[0;36mfndict\u001b[0;34m(filename, dct)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mfunction_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'def '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'('\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearchParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m#Search lines above and below to identify comments, store the comments into a string and add do dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-485000a3f14a>\u001b[0m in \u001b[0;36msearchParams\u001b[0;34m(lines, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'('\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m')'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mparam_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparam_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "fndict('memonger.txt', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmemonger.py - throws error, working on fixing it\\nattention.py (contains comments above header)\\ncontext.py - passed\\ncontrol_ops_grad.py - passes but has variable including \"def\", so it falsely adds \"= _prepare_gradient_do_op\" \\n    as a function in the dict, need to implement that \\ndyndep.py - passed\\nFunDict_tc1.txt - passed\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test cases\n",
    "'''\n",
    "memonger.py - throws error, working on fixing it\n",
    "attention.py (contains comments above header)\n",
    "context.py - passed\n",
    "control_ops_grad.py - passes but has variable including \"def\", so it falsely adds \"= _prepare_gradient_do_op\" \n",
    "    as a function in the dict, need to implement that \n",
    "dyndep.py - passed\n",
    "FunDict_tc1.txt - passed\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
